{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CNN-Based Classification of Cats vs. Dogs from Image Data\n",
    "\n",
    "This project leverages a convolutional neural network (CNN) to classify images of dogs and cats. The dataset was obtained from Kaggle and preprocessed for use in a deep learning model built with TensorFlow and Keras.\n",
    "\n",
    "The project demonstrates the complete workflow of loading and preparing image data, building a CNN architecture, applying data augmentation and regularization techniques, and evaluating model performance using accuracy and loss visualizations.\n",
    "\n",
    "Dataset Details: https://www.kaggle.com/datasets/salader/dogs-vs-cats\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kbV_qJu4Rc2D"
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "import subprocess\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "from keras.models import Sequential as KerasSequential\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìÅ Dataset Preparation: Downloading and Extracting Kaggle Dataset\n",
    "\n",
    "\n",
    "We begin by setting up Kaggle API access to download the 'Dogs vs. Cats' dataset. We use Python‚Äôs built-in libraries to authenticate and download the ZIP archive, which is then extracted for use in model training.\n",
    "\n",
    "Steps:\n",
    "1. Copy `kaggle.json` to the appropriate directory.\n",
    "2. Download the dataset using `kaggle datasets download`.\n",
    "3. Unzip the downloaded file for training and validation usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Set up Kaggle\n",
    "kaggle_json = 'kaggle.json'\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "\n",
    "if not os.path.exists(kaggle_dir):\n",
    "    os.makedirs(kaggle_dir)\n",
    "\n",
    "# Copy and set permissions\n",
    "shutil.copy(kaggle_json, os.path.join(kaggle_dir, 'kaggle.json'))\n",
    "os.chmod(os.path.join(kaggle_dir, 'kaggle.json'), 0o600)\n",
    "\n",
    "# Download the dataset using Kaggle\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "subprocess.run(['kaggle', 'datasets', 'download', '-d', 'salader/dogs-vs-cats'], check=True)\n",
    "\n",
    "# Unzip the downloaded dataset\n",
    "zip_path = 'dogs-vs-cats.zip'\n",
    "extract_path = 'dogs-vs-cats'\n",
    "\n",
    "print(f\"Unzipping dataset to: {extract_path}\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"Download and extraction complete!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "E3GARcHhTDxx",
    "outputId": "251bb0f6-17c7-4318-e509-85a6ad3e4ed0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚öôÔ∏è Configuration & Data Loading\n",
    "\n",
    "\n",
    "We configure important parameters which includes batch size, image dimensions, and the number of epochs. We use `image_dataset_from_directory` to load images from the extracted dataset directory."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (256, 256)\n",
    "TRAIN_DIR = os.path.join(\"dogs-vs-cats\", \"train\")\n",
    "TEST_DIR = os.path.join(\"dogs-vs-cats\", \"test\")\n",
    "EPOCHS = 10\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------------\n",
    "def load_dataset(directory):\n",
    "    return keras.utils.image_dataset_from_directory(\n",
    "        directory=directory,\n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "print(\"Loading training dataset...\")\n",
    "train_ds = load_dataset(TRAIN_DIR)\n",
    "\n",
    "print(\"Loading validation dataset...\")\n",
    "validation_ds = load_dataset(TEST_DIR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üßº Preprocessing: Normalization\n",
    "\n",
    "Pixel values are normalized to the range [0, 1] for better numerical stability during training. Data augmentation is applied to the training data to reduce overfitting by randomly flipping and rotating images."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# DATA AUGMENTATION & NORMALIZATION\n",
    "# -------------------------------\n",
    "data_augmentation = KerasSequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1)\n",
    "])\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y)).map(preprocess)\n",
    "validation_ds = validation_ds.map(preprocess)\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.prefetch(buffer_size=AUTOTUNE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß† Model Architecture: CNN\n",
    "\n",
    "Defined a convolutional neural network with three convolutional layers, each followed by batch normalization and max pooling.\n",
    "\n",
    "Dense layers follow the feature extractor, ending in a sigmoid layer for binary classification.\n",
    "\n",
    "Improvements for generalization:\n",
    "- Dropout layers\n",
    "- BatchNormalization"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# BUILD CNN MODEL\n",
    "# -------------------------------\n",
    "model = Sequential([\n",
    "    Input(shape=(256, 256, 3)),\n",
    "\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='valid'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='valid'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='valid'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üõ†Ô∏è Compile & Train the Model\n",
    "\n",
    "Compiled the model with the Adam optimizer and binary cross-entropy loss.\n",
    "Early stopping is implemented to halt training if validation loss does not improve, helping reduce overfitting."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# COMPILE\n",
    "# -------------------------------\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# CALLBACKS\n",
    "# -------------------------------\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss', verbose=1)\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# TRAIN MODEL\n",
    "# -------------------------------\n",
    "print(\"Starting training with augmentation and callbacks...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_ds,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìà Visualization: Accuracy and Loss Trends\n",
    "\n",
    "Plotting training and validation accuracy/loss across epochs allows us to visually inspect model performance and check for overfitting."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# VISUALIZE PERFORMANCE\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='red')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='red')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='blue')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Discussion & Conclusion\n",
    "\n",
    "This project aimed to classify images of cats and dogs using a convolutional neural network (CNN), demonstrating how model performance can be improved through strategic enhancements and evaluated through key metrics such as accuracy and loss.\n",
    "\n",
    "The model initially started with modest accuracy and high loss, indicating room for optimization. To improve performance, we implemented several key strategies:\n",
    "- **Data Augmentation** was applied to increase dataset variability and reduce overfitting.\n",
    "- **Batch Normalization** layers were added to stabilize and accelerate training.\n",
    "- **Dropout** was introduced to reduce the risk of overfitting.\n",
    "\n",
    "The **accuracy plot** reveals a steady increase in training accuracy over epochs, with validation accuracy initially improving in parallel. However, fluctuations in validation accuracy, especially after epoch 7, suggest possible overfitting or instability in generalization.\n",
    "\n",
    "The **loss plot** corroborates this interpretation. Training loss decreased consistently, indicating effective learning, while the validation loss showed a rise and variability after epoch 6. This divergence is a typical sign of overfitting, where the model learns the training data too specifically and struggles to generalize.\n",
    "\n",
    "In summary, this project provided entry experience in designing and refining a CNN model for image classification. By monitoring performance curves and iteratively adjusting architecture and hyperparameters, we observed tangible improvements in model behavior and learned key practices in deep learning model development and evaluation."
   ]
  }
 ]
}
